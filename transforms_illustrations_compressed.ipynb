{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o722SuRWUvrf"
   },
   "source": [
    "\n",
    "# Illustration of transforms\n",
    "\n",
    "\n",
    "This example illustrates some of the various transforms available in `the\n",
    "torchvision.transforms.v2 module <transforms>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o8yricopU6PC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot(images, row_title=None, **imshow_kwargs):\n",
    "    \"\"\"Displays a flat list or nested list of images or (image, boxes) pairs.\"\"\"\n",
    "\n",
    "    # Normalize to grid format: list of rows\n",
    "    if isinstance(images[0], (list, tuple)) and not isinstance(images[0][0], (torch.Tensor, tuple)):\n",
    "        grid = images  # already a grid\n",
    "    else:\n",
    "        grid = [images]  # convert flat to single-row grid\n",
    "\n",
    "    num_rows = len(grid)\n",
    "    num_cols = max(len(row) for row in grid)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 5 * num_rows), squeeze=False)\n",
    "\n",
    "    for i, row in enumerate(grid):\n",
    "        for j, item in enumerate(row):\n",
    "            ax = axs[i][j]\n",
    "            if isinstance(item, tuple):\n",
    "                img, boxes = item\n",
    "            else:\n",
    "                img, boxes = item, None\n",
    "\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.detach()\n",
    "                if img.ndim == 3 and img.shape[0] in (1, 3):\n",
    "                    img = img.permute(1, 2, 0)  # C x H x W → H x W x C\n",
    "\n",
    "            ax.imshow(img, **imshow_kwargs)\n",
    "            ax.axis('off')\n",
    "\n",
    "            if boxes is not None:\n",
    "                for box in boxes.to('cpu'):\n",
    "                    x0, y0, x1, y1 = box.tolist()\n",
    "                    rect = plt.Rectangle((x0, y0), x1 - x0, y1 - y0,\n",
    "                                         fill=False, edgecolor='red', linewidth=2)\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "        if row_title and i < len(row_title):\n",
    "            axs[i][0].set_ylabel(row_title[i], fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YnVjsXzVj5L",
    "outputId": "73637e46-72bd-46fc-86ce-26107e01a59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘assets’: File exists\n",
      "--2025-05-29 14:14:30--  https://github.com/pytorch/vision/raw/main/gallery/assets/astronaut.jpg\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/astronaut.jpg [following]\n",
      "--2025-05-29 14:14:31--  https://raw.githubusercontent.com/pytorch/vision/main/gallery/assets/astronaut.jpg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40344 (39K) [image/jpeg]\n",
      "Saving to: ‘./assets/astronaut.jpg’\n",
      "\n",
      "./assets/astronaut. 100%[===================>]  39.40K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-05-29 14:14:31 (2.12 MB/s) - ‘./assets/astronaut.jpg’ saved [40344/40344]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir assets\n",
    "!wget https://github.com/pytorch/vision/raw/main/gallery/assets/astronaut.jpg -O ./assets/astronaut.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jvcY8lwlUvrk"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "# if you change the seed, make sure that the randomly-applied transforms\n",
    "# properly show that the image can be both transformed and *not* transformed!\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# If you're trying to run that on Colab, you can download the assets and the\n",
    "# helpers from https://github.com/pytorch/vision/tree/main/gallery/\n",
    "# from helpers import plot\n",
    "\n",
    "orig_img = Image.open(Path('./assets') / 'astronaut.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jin1He5gUvrm"
   },
   "source": [
    "## Geometric Transforms\n",
    "Geometric image transformation refers to the process of altering the geometric properties of an image,\n",
    "such as its shape, size, orientation, or position.\n",
    "It involves applying mathematical operations to the image pixels or coordinates to achieve the desired transformation.\n",
    "\n",
    "### Pad\n",
    "The :class:`~torchvision.transforms.Pad` transform\n",
    "(see also :func:`~torchvision.transforms.functional.pad`)\n",
    "pads all image borders with some pixel values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "NRxgKupSUvrn",
    "outputId": "ae3d4dd9-b6bf-4896-edc0-919f601caa77"
   },
   "outputs": [],
   "source": [
    "padded_imgs = [v2.Pad(padding=padding)(orig_img) for padding in (3, 10, 30, 50)]\n",
    "plot([orig_img] + padded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa2f0d",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32g9TGr4Uvro"
   },
   "source": [
    "### Resize\n",
    "The :class:`~torchvision.transforms.Resize` transform\n",
    "(see also :func:`~torchvision.transforms.functional.resize`)\n",
    "resizes an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "Z-XuwNHLUvro",
    "outputId": "16c44b71-3817-4214-94d1-1309634f2529"
   },
   "outputs": [],
   "source": [
    "resized_imgs = [v2.Resize(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n",
    "plot([orig_img] + resized_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754fe83",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ3LrFR0Uvro"
   },
   "source": [
    "### CenterCrop\n",
    "The :class:`~torchvision.transforms.CenterCrop` transform\n",
    "(see also :func:`~torchvision.transforms.functional.center_crop`)\n",
    "crops the given image at the center.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "iS8rpr6aUvrp",
    "outputId": "09f45d9b-3393-45c1-ca2b-51182c6f3d55"
   },
   "outputs": [],
   "source": [
    "center_crops = [v2.CenterCrop(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n",
    "plot([orig_img] + center_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46552209",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G52C6DtjUvrp"
   },
   "source": [
    "### FiveCrop\n",
    "The :class:`~torchvision.transforms.FiveCrop` transform\n",
    "(see also :func:`~torchvision.transforms.functional.five_crop`)\n",
    "crops the given image into four corners and the central crop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "YpAdwSlCUvrp",
    "outputId": "468208f8-0e62-4bf8-ff4d-f4f64e436856"
   },
   "outputs": [],
   "source": [
    "(top_left, top_right, bottom_left, bottom_right, center) = v2.FiveCrop(size=(100, 100))(orig_img)\n",
    "plot([orig_img] + [top_left, top_right, bottom_left, bottom_right, center])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb75de3",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79siVcJcUvrq"
   },
   "source": [
    "### RandomPerspective\n",
    "The :class:`~torchvision.transforms.RandomPerspective` transform\n",
    "(see also :func:`~torchvision.transforms.functional.perspective`)\n",
    "performs random perspective transform on an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "Fs6QWJ6RUvrq",
    "outputId": "65c34435-e656-4718-bb5c-6b65f2b8ecc7"
   },
   "outputs": [],
   "source": [
    "perspective_transformer = v2.RandomPerspective(distortion_scale=0.6, p=1.0)\n",
    "perspective_imgs = [perspective_transformer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + perspective_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec92dd0",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKtWJuEJUvrq"
   },
   "source": [
    "### RandomRotation\n",
    "The :class:`~torchvision.transforms.RandomRotation` transform\n",
    "(see also :func:`~torchvision.transforms.functional.rotate`)\n",
    "rotates an image with random angle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "0mAwBMJOUvrr",
    "outputId": "fc51d0a3-b00b-4619-dbe8-9f7217d88a65"
   },
   "outputs": [],
   "source": [
    "rotater = v2.RandomRotation(degrees=(0, 180))\n",
    "rotated_imgs = [rotater(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + rotated_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81a12e",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rsfka9AbUvrr"
   },
   "source": [
    "### RandomAffine\n",
    "The :class:`~torchvision.transforms.RandomAffine` transform\n",
    "(see also :func:`~torchvision.transforms.functional.affine`)\n",
    "performs random affine transform on an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "6RNbi3OWUvrs",
    "outputId": "bb64dcc9-0707-4d65-8cab-c7edfc0e788f"
   },
   "outputs": [],
   "source": [
    "affine_transfomer = v2.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))\n",
    "affine_imgs = [affine_transfomer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + affine_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6aac6",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd0AcDYnUvrs"
   },
   "source": [
    "### ElasticTransform\n",
    "The :class:`~torchvision.transforms.ElasticTransform` transform\n",
    "(see also :func:`~torchvision.transforms.functional.elastic_transform`)\n",
    "Randomly transforms the morphology of objects in images and produces a\n",
    "see-through-water-like effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "171RSEPVUvrs",
    "outputId": "d151c76b-1070-4940-d12a-8e17351fe5e1"
   },
   "outputs": [],
   "source": [
    "elastic_transformer = v2.ElasticTransform(alpha=250.0)\n",
    "transformed_imgs = [elastic_transformer(orig_img) for _ in range(2)]\n",
    "plot([orig_img] + transformed_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f64c8",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p7s9GoiUvrt"
   },
   "source": [
    "### RandomCrop\n",
    "The :class:`~torchvision.transforms.RandomCrop` transform\n",
    "(see also :func:`~torchvision.transforms.functional.crop`)\n",
    "crops an image at a random location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "eeCrbM7GUvrt",
    "outputId": "8b8726cb-5632-4f2b-c567-3d8879613570"
   },
   "outputs": [],
   "source": [
    "cropper = v2.RandomCrop(size=(128, 128))\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628dbda0",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GHVk3iKUvrt"
   },
   "source": [
    "### RandomResizedCrop\n",
    "The :class:`~torchvision.transforms.RandomResizedCrop` transform\n",
    "(see also :func:`~torchvision.transforms.functional.resized_crop`)\n",
    "crops an image at a random location, and then resizes the crop to a given\n",
    "size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "I9Pizv2tUvru",
    "outputId": "ee562b01-b309-4b7f-c88e-1236617d82e2"
   },
   "outputs": [],
   "source": [
    "resize_cropper = v2.RandomResizedCrop(size=(32, 32))\n",
    "resized_crops = [resize_cropper(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + resized_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68137ba",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blTXVtA3Uvru"
   },
   "source": [
    "## Photometric Transforms\n",
    "Photometric image transformation refers to the process of modifying the photometric properties of an image,\n",
    "such as its brightness, contrast, color, or tone.\n",
    "These transformations are applied to change the visual appearance of an image\n",
    "while preserving its geometric structure.\n",
    "\n",
    "Except :class:`~torchvision.transforms.Grayscale`, the following transforms are random,\n",
    "which means that the same transform\n",
    "instance will produce different result each time it transforms a given image.\n",
    "\n",
    "### Grayscale\n",
    "The :class:`~torchvision.transforms.Grayscale` transform\n",
    "(see also :func:`~torchvision.transforms.functional.to_grayscale`)\n",
    "converts an image to grayscale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "LWZ1U0OSUvru",
    "outputId": "e166fe7b-1512-4064-a71a-07410bc09b51"
   },
   "outputs": [],
   "source": [
    "gray_img = v2.Grayscale()(orig_img)\n",
    "plot([orig_img, gray_img], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420046b",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6wiGRtgUvru"
   },
   "source": [
    "### ColorJitter\n",
    "The :class:`~torchvision.transforms.ColorJitter` transform\n",
    "randomly changes the brightness, contrast, saturation, hue, and other properties of an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "f9HJtb5wUvru",
    "outputId": "6fd53102-bbf0-4c78-90a6-eb6642c0d273"
   },
   "outputs": [],
   "source": [
    "jitter = v2.ColorJitter(brightness=.5, hue=.3)\n",
    "jittered_imgs = [jitter(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + jittered_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473240eb",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL9UcHZNUvrv"
   },
   "source": [
    "### GaussianBlur\n",
    "The :class:`~torchvision.transforms.GaussianBlur` transform\n",
    "(see also :func:`~torchvision.transforms.functional.gaussian_blur`)\n",
    "performs gaussian blur transform on an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "z8QV2ZJWUvrv",
    "outputId": "503077ee-de62-4103-a8fd-98d3e1e7a3e4"
   },
   "outputs": [],
   "source": [
    "blurrer = v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))\n",
    "blurred_imgs = [blurrer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + blurred_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d51c9",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0S6akCNUvrv"
   },
   "source": [
    "### RandomInvert\n",
    "The :class:`~torchvision.transforms.RandomInvert` transform\n",
    "(see also :func:`~torchvision.transforms.functional.invert`)\n",
    "randomly inverts the colors of the given image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "3j1TSsliUvrv",
    "outputId": "4a7d9fb8-a169-427c-bccb-81d4ea99aa7d"
   },
   "outputs": [],
   "source": [
    "inverter = v2.RandomInvert()\n",
    "invertered_imgs = [inverter(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + invertered_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b289e5",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGwhCmuqUvrv"
   },
   "source": [
    "### RandomPosterize\n",
    "The :class:`~torchvision.transforms.RandomPosterize` transform\n",
    "(see also :func:`~torchvision.transforms.functional.posterize`)\n",
    "randomly posterizes the image by reducing the number of bits\n",
    "of each color channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "IjnQWMo_Uvrv",
    "outputId": "5e59e64f-8841-4038-fb36-7b6b3c298c8a"
   },
   "outputs": [],
   "source": [
    "posterizer = v2.RandomPosterize(bits=2)\n",
    "posterized_imgs = [posterizer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + posterized_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce4b86",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tLcv3JIUvrw"
   },
   "source": [
    "### RandomSolarize\n",
    "The :class:`~torchvision.transforms.RandomSolarize` transform\n",
    "(see also :func:`~torchvision.transforms.functional.solarize`)\n",
    "randomly solarizes the image by inverting all pixel values above\n",
    "the threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "dZj1SAK5Uvrw",
    "outputId": "db70c45a-6125-44c4-de31-84ee51c562cc"
   },
   "outputs": [],
   "source": [
    "solarizer = v2.RandomSolarize(threshold=192.0)\n",
    "solarized_imgs = [solarizer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + solarized_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f696ef",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyvzPAz_Uvrw"
   },
   "source": [
    "### RandomAdjustSharpness\n",
    "The :class:`~torchvision.transforms.RandomAdjustSharpness` transform\n",
    "(see also :func:`~torchvision.transforms.functional.adjust_sharpness`)\n",
    "randomly adjusts the sharpness of the given image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "0k1RygnIUvrw",
    "outputId": "7dd6e0a3-3dc3-4b44-8c50-670e9b8eb3c4"
   },
   "outputs": [],
   "source": [
    "sharpness_adjuster = v2.RandomAdjustSharpness(sharpness_factor=2)\n",
    "sharpened_imgs = [sharpness_adjuster(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + sharpened_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a88069",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G7GL4OMUvrw"
   },
   "source": [
    "### RandomAutocontrast\n",
    "The :class:`~torchvision.transforms.RandomAutocontrast` transform\n",
    "(see also :func:`~torchvision.transforms.functional.autocontrast`)\n",
    "randomly applies autocontrast to the given image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "_B-WwPZDUvrw",
    "outputId": "e697512e-ccde-4c7d-b261-753b66c691df"
   },
   "outputs": [],
   "source": [
    "autocontraster = v2.RandomAutocontrast()\n",
    "autocontrasted_imgs = [autocontraster(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + autocontrasted_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd387f",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuRUd9KhUvrx"
   },
   "source": [
    "### RandomEqualize\n",
    "The :class:`~torchvision.transforms.RandomEqualize` transform\n",
    "(see also :func:`~torchvision.transforms.functional.equalize`)\n",
    "randomly equalizes the histogram of the given image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "e86KdkelUvr1",
    "outputId": "e3bddbcc-c721-407a-861b-65db9049408e"
   },
   "outputs": [],
   "source": [
    "equalizer = v2.RandomEqualize()\n",
    "equalized_imgs = [equalizer(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + equalized_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde55a47",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_18.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCFmP7nkUvr2"
   },
   "source": [
    "### JPEG\n",
    "The :class:`~torchvision.transforms.v2.JPEG` transform\n",
    "(see also :func:`~torchvision.transforms.v2.functional.jpeg`)\n",
    "applies JPEG compression to the given image with random\n",
    "degree of compression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "kf5R3ePjUvr2",
    "outputId": "3daaf1f2-4a79-4e36-8689-871644fcd50e"
   },
   "outputs": [],
   "source": [
    "jpeg = v2.JPEG((5, 50))\n",
    "jpeg_imgs = [jpeg(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + jpeg_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9011a70",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f35esz-Uvr2"
   },
   "source": [
    "## Augmentation Transforms\n",
    "The following transforms are combinations of multiple transforms,\n",
    "either geometric or photometric, or both.\n",
    "\n",
    "### AutoAugment\n",
    "The :class:`~torchvision.transforms.AutoAugment` transform\n",
    "automatically augments data based on a given auto-augmentation policy.\n",
    "See :class:`~torchvision.transforms.AutoAugmentPolicy` for the available policies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "id": "WrTsWNcfUvr2",
    "outputId": "146b316d-0229-4291-a7ca-f7f06145cfc4"
   },
   "outputs": [],
   "source": [
    "policies = [v2.AutoAugmentPolicy.CIFAR10, v2.AutoAugmentPolicy.IMAGENET, v2.AutoAugmentPolicy.SVHN]\n",
    "augmenters = [v2.AutoAugment(policy) for policy in policies]\n",
    "imgs = [\n",
    "    [augmenter(orig_img) for _ in range(4)]\n",
    "    for augmenter in augmenters\n",
    "]\n",
    "row_title = [str(policy).split('.')[-1] for policy in policies]\n",
    "plot([[orig_img] + row for row in imgs], row_title=row_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc149c9f",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_20.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56772r-mUvr2"
   },
   "source": [
    "### RandAugment\n",
    "The :class:`~torchvision.transforms.RandAugment` is an alternate version of AutoAugment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "1VlJQR4SUvr3",
    "outputId": "ecb97529-d690-431e-c9ba-271c3dce5f16"
   },
   "outputs": [],
   "source": [
    "augmenter = v2.RandAugment()\n",
    "imgs = [augmenter(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859a971",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_21.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJsxcBB5Uvr3"
   },
   "source": [
    "### TrivialAugmentWide\n",
    "The :class:`~torchvision.transforms.TrivialAugmentWide` is an alternate implementation of AutoAugment.\n",
    "However, instead of transforming an image multiple times, it transforms an image only once\n",
    "using a random transform from a given list with a random strength number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "3R6dw-6MUvr3",
    "outputId": "05c0e278-b1c1-48df-a8e1-c300759e9d09"
   },
   "outputs": [],
   "source": [
    "augmenter = v2.TrivialAugmentWide()\n",
    "imgs = [augmenter(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f84bf8",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_22.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbRGgR5rUvr3"
   },
   "source": [
    "### AugMix\n",
    "The :class:`~torchvision.transforms.AugMix` transform interpolates between augmented versions of an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "f245Gau-Uvr3",
    "outputId": "1669ab4d-468f-4266-995d-dadc40de3cba"
   },
   "outputs": [],
   "source": [
    "augmenter = v2.AugMix()\n",
    "imgs = [augmenter(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd4d01",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_23.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdPELOVKUvr3"
   },
   "source": [
    "## Randomly-applied Transforms\n",
    "\n",
    "The following transforms are randomly-applied given a probability ``p``.  That is, given ``p = 0.5``,\n",
    "there is a 50% chance to return the original image, and a 50% chance to return the transformed image,\n",
    "even when called with the same transform instance!\n",
    "\n",
    "### RandomHorizontalFlip\n",
    "The :class:`~torchvision.transforms.RandomHorizontalFlip` transform\n",
    "(see also :func:`~torchvision.transforms.functional.hflip`)\n",
    "performs horizontal flip of an image, with a given probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "qpE9wwf1Uvr3",
    "outputId": "757eb071-f884-47de-d8cb-0ce3a130f903"
   },
   "outputs": [],
   "source": [
    "hflipper = v2.RandomHorizontalFlip(p=0.5)\n",
    "transformed_imgs = [hflipper(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + transformed_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1c0e0",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6uuExaZUvr3"
   },
   "source": [
    "### RandomVerticalFlip\n",
    "The :class:`~torchvision.transforms.RandomVerticalFlip` transform\n",
    "(see also :func:`~torchvision.transforms.functional.vflip`)\n",
    "performs vertical flip of an image, with a given probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "kNd9EUJ4Uvr4",
    "outputId": "ef3e97a4-11fd-448c-f512-62bb6389786c"
   },
   "outputs": [],
   "source": [
    "vflipper = v2.RandomVerticalFlip(p=0.5)\n",
    "transformed_imgs = [vflipper(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + transformed_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e47c2f",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f8vMLjcUvr4"
   },
   "source": [
    "### RandomApply\n",
    "The :class:`~torchvision.transforms.RandomApply` transform\n",
    "randomly applies a list of transforms, with a given probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "t6iT1YQ4Uvr4",
    "outputId": "039684d3-1be5-4b40-abfb-94f1d82014c5"
   },
   "outputs": [],
   "source": [
    "applier = v2.RandomApply(transforms=[v2.RandomCrop(size=(64, 64))], p=0.5)\n",
    "transformed_imgs = [applier(orig_img) for _ in range(4)]\n",
    "plot([orig_img] + transformed_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf6503",
   "metadata": {},
   "source": [
    "![output image](notebook_images\\img_26.jpg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
